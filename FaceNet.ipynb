{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FaceNet\n",
    "In this notebook, we are going to analyse the FaceNet paper(Schroff et al.). First let's embed the pdf and try to summarise it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"980\"\n",
       "            height=\"900\"\n",
       "            src=\"https://arxiv.org/pdf/1503.03832.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f405416d2e8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(\"https://arxiv.org/pdf/1503.03832.pdf\", width=980, height=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "\n",
    "1. The paper introduces a novel loss function, called triplet loss function, motivated specifically by the task at hand: irrespective of pose/illumination, similar faces should be closer to each other than dissimmilar faces.\n",
    "2. To this end, a CNN based on Zeiler-Fergus paper is trained (with additional tweakings)\n",
    "3. The output of the CNN is then L2-normalised so that the image lies in an embedded space: on a surface of a 128-dimensional hypersphere.\n",
    "\n",
    "4. These embeddings of images are used to construct a triplet loss function, only those triplets which can improve learning are used for optimization: a= embedding of an anchor, p= embedding of a positive, n= embedding of a negative. Whenever $$\\mathbb{L}_2(a-n) < \\mathbb{L}_2(a-p), \\qquad for \\  a \\  triple \\  (a,p,n) \\in \\mathbb{\\tau}$$ it is used to calculate loss:$$\\mathscr{L} = \\sum_i^N [\\mathbb{L}_2^2(a-p)+\\alpha-\\mathbb{L}_2^2(a-n)]_{+}$$ \n",
    "5. The optimizer then tries to minimize the error by forcing n **away** from a and by **bringing** p closer to a at least by a margin of alpha.\n",
    "6. Once the whole network along with the Euclidean embedding is optimized, a simple L2 distance threshold is used to classify faces as same or different.\n",
    "\n",
    "                ############################ END of summary ####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideally, what can we do about the FaceNet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We can implement the NNx/NNSx architecture to minimize the embedding loss according to the triple loss and learn the hyperparameter D, the L2 distance separator of faces.\n",
    "2. Using the weights and D, we can verify whether two faces are same or not.\n",
    "3. We can recognize a face by k-NN\n",
    "4. We can cluster similar faces using standard clustering techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the most interesting extension would be applying t-SNE on embedding to visualise the results, to verify whether similar face are really visually similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sections cover a simplistic approach by which the model could be created, trained and used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create NN1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will create the NN1 architecture. The architecture here is slightly different that the one used in paper, for we implement ReLU+dropout instead of **maxout** for all the fully-connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import tensorflow as tf\n",
    "\n",
    "class FaceNet(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def convoluteR(self, inputs, filters, kernel_size, strides,padding=\"same\", activation=tf.nn.relu ):\n",
    "        #conv+ReLU\n",
    "        return tf.layers.conv2d(inputs=inputs, filters=filters, kernel_size=kernel_size,\\\n",
    "                                strides=strides, padding=padding, activation=activation)\n",
    "    \n",
    "    def pool(self, inputs, pool_size, strides, padding=\"same\"):\n",
    "        return tf.layers.max_pooling2d(inputs=inputs, pool_size=pool_size,\\\n",
    "                                       strides=strides, padding=padding)\n",
    "    \n",
    "    def lrn(self, inputs, radius=1,alpha=2e-05, beta=0.75, bias=1.0):\n",
    "        return tf.nn.local_response_normalization(inputs, depth_radius = radius,\\\n",
    "                                                  alpha = alpha, beta = beta, bias = bias)\n",
    "    def fc(self, inputs, units, activation=tf.nn.relu, drop=0.5, mode=True):\n",
    "        inputs = tf.reshape(inputs, [-1, reduce(lambda x, y: x*y, inputs.get_shape().as_list()[1:])])\n",
    "        _fc = tf.layers.dense(inputs=inputs, units=reduce(lambda x, y: x*y, units), activation=activation)\n",
    "        out = tf.layers.dropout(inputs=_fc, rate=0.5, training=mode)\n",
    "        return tf.reshape(out, [-1]+units)\n",
    "    def create(self, inputs):\n",
    "        #layer1\n",
    "        conv1 = self.convoluteR(inputs=inputs, filters=64, kernel_size=[7,7], strides=2)\n",
    "        pool1 = self.pool(inputs=conv1,pool_size=[3,3],strides=2)\n",
    "        rnorm1 = self.lrn(inputs=pool1)\n",
    "        \n",
    "        #layer2\n",
    "        conv2a = self.convoluteR(inputs=rnorm1, filters=64, kernel_size=[1,1], strides=1)\n",
    "        conv2 = self.convoluteR(inputs=conv2a, filters=192, kernel_size=[3,3], strides=1)\n",
    "        rnorm2 = self.lrn(inputs=conv2)\n",
    "        pool2 = self.pool(inputs=rnorm2, pool_size=[3,3], strides=2)\n",
    "        \n",
    "        #layer3\n",
    "        conv3a = self.convoluteR(inputs=pool2, filters=192, kernel_size=[1,1], strides=1)\n",
    "        conv3 = self.convoluteR(inputs=conv3a, filters=384, kernel_size=[3,3], strides=1)\n",
    "        pool3 = self.pool(inputs=conv3, pool_size=[3,3], strides=2)\n",
    "        \n",
    "        #layer4\n",
    "        conv4a = self.convoluteR(inputs=pool3, filters=384, kernel_size=[1,1], strides=1)\n",
    "        conv4 = self.convoluteR(inputs=conv4a, filters=256, kernel_size=[3,3], strides=1)\n",
    "        \n",
    "        #layer5\n",
    "        conv5a = self.convoluteR(inputs=conv4,filters=256, kernel_size=[1,1], strides=1)\n",
    "        conv5 = self.convoluteR(inputs=conv5a, filters=256, kernel_size=[3,3], strides=1)\n",
    "        \n",
    "        #layer6\n",
    "        conv6a = self.convoluteR(inputs=conv5, filters=256, kernel_size=[1,1], strides=1)\n",
    "        conv6 = self.convoluteR(inputs=conv6a, filters=256, kernel_size=[3,3], strides=1)\n",
    "        pool4 = self.pool(inputs=conv6, pool_size=[3,3], strides=2)\n",
    "        \n",
    "        #fc\n",
    "        f1 = self.fc(pool4, units=[1,32,128])\n",
    "        f2 = self.fc(f1,units=[1,32,128])\n",
    "        f3 = self.fc(f2,units=[1,1,128])\n",
    "        \n",
    "        #l2 sphere\n",
    "        l2 = tf.divide(f3, tf.sqrt(tf.reduce_sum(tf.square(f3))))\n",
    "        #self.embeddings = l2\n",
    "        return l2\n",
    "    \n",
    "    def triplet_loss(self, triplet, alpha=0.2):\n",
    "        anchor, positive, negative = triplet\n",
    "        p_diff = tf.reduce_sum(tf.square(tf.add(anchor, tf.negative(positive))), axis=-1)\n",
    "        n_diff = tf.reduce_sum(tf.square(tf.add(anchor, tf.negative(negative))), axis=-1)\n",
    "        diff = tf.add(tf.add(p_diff,tf.negative(n_diff)), alpha)\n",
    "        loss = tf.reduce_sum(tf.reduce_sum(tf.maximum(diff, 0.0), axis=0))\n",
    "        return loss\n",
    "    #not for training\n",
    "    def infer(self, inputs):\n",
    "        return self.create(inputs)\n",
    "    def train(self,loss):\n",
    "        train_op = tf.contrib.layers.optimize_loss(loss=loss,global_step=tf.contrib.framework.get_global_step(),\\\n",
    "                                                learning_rate=0.001,optimizer=\"Adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = '/path/to/train/folder/' #contains images from datasets like LFW dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, random, itertools\n",
    "import numpy as np\n",
    "def get_triplets(url):\n",
    "    faces = random.sample(os.listdir(url),2)\n",
    "    pos = os.path.join(url, faces[0],'face')\n",
    "    neg = os.path.join(url, faces[1],'face')\n",
    "    url_triplet = [[pos+'/'+x[0], pos+'/'+x[1],neg+'/'+random.sample(os.listdir(neg),1)[0]] for x in itertools.combinations(random.sample(os.listdir(pos),6), 2)]\n",
    "    url_triplet = sum(url_triplet,[])\n",
    "    return pos, neg, url_triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos, neg, triplet = get_triplets(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(triplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_img(triplet):\n",
    "    #triplet is a list of urls\n",
    "    arr = []\n",
    "    for x in triplet:\n",
    "        arr.append(np.array(Image.open(x).resize((220,220))))\n",
    "    return np.array(arr, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr = read_img(triplet[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 220, 220, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p, q, r = tf.placeholder(tf.float32, shape=[None,220,220,3]),tf.placeholder(tf.float32, shape=[None,220,220,3]),tf.placeholder(tf.float32, shape=[None,220,220,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn = FaceNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "anchor = nn.create(tf.reshape(p,[-1,220,220,3]))\n",
    "positive = nn.create(tf.reshape(q,[-1,220,220,3]))\n",
    "negative = nn.create(tf.reshape(r,[-1,220,220,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = nn.triplet_loss((anchor, positive, negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.AdamOptimizer(0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current batch loss [[ 0.18999195]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1024):\n",
    "    _,_, triplet = get_triplets(url)\n",
    "    arr = read_img(triplet[:15])\n",
    "    sess.run(train_step, feed_dict={p:arr[0::3], q:arr[1::3], r:arr[2::3]})\n",
    "    if i%100==0:\n",
    "        print(\"current batch loss %s\" % sess.run(loss, feed_dict={p:arr[0::3], q:arr[1::3], r:arr[2::3]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
